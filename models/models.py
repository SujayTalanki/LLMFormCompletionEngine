"""
This file contains the Replicate/OpenAI objects for the different LLM's
"""

import replicate
from llama_index.llms.replicate import Replicate
from llama_index.llms.openai import OpenAI
from src.prompts import (
    form_completion_prompt,
    interface_form_completion_prompt,
    feedback_prompt,
)


# Class to build Replicate object given a model path
class Model:
    """
    A class to represent a model for form completion tasks.

    Attributes:
        model_path (str): Path to the model.
        additional_args (dict): Additional arguments for the model.
        temperature (float): Temperature for controlling randomness of the
                             model.
        system_prompt (str): Default prompt used in model interactions.
        llm (Replicate): An object to handle model inference through Replicate.
    """

    def __init__(
        self,
        model_path,
        additional_args,
        temperature=0.1,
        system_prompt=form_completion_prompt,
    ):
        """
        Initializes the Model class with the given parameters.

        Args:
            model_path (str): The path to the model on Replicate or locally.
            additional_args (dict): Additional keyword arguments for inference.
            temperature (float, optional): Randomness control, default is 0.1.
            system_prompt (str, optional): System prompt for model guidance.
                Defaults to `form_completion_prompt`.
        """
        self.model_path = model_path
        self.temperature = temperature
        self.additional_args = additional_args
        self.system_prompt = system_prompt
        self.llm = self.create_replicate_object()

    def create_replicate_object(self):
        """
        Creates and returns a Replicate object with the model configuration.

        Args:
            None.

        Returns:
            Replicate: A Replicate object initialized with the model path,
                temperature, and additional arguments.
        """
        return Replicate(
            model=self.model_path,
            temperature=self.temperature,
            additional_kwargs=self.additional_args,
            system_prompt=self.system_prompt,
        )


class FeedbackModel:
    """
    A class to represent a feedback model using Replicate API.

    Attributes:
        model_path (str): Path to the model on Replicate.
        temperature (float): Temperature for controlling randomness of the
                             model.
    """

    def __init__(
        self,
        model_path="meta/meta-llama-3-8b-instruct",
        temperature=0.7,
    ):
        """
        Initializes the FeedbackModel class with the given model path and
        temperature.

        Args:
            model_path (str, optional): The path to the model on Replicate.
                Defaults to "meta/meta-llama-3-8b-instruct".
            temperature (float, optional): Randomness control, default is 0.7.
        """
        self.model_path = model_path
        self.temperature = temperature

    def run_model(self, system_prompt, prompt):
        """
        Runs the feedback model on Replicate with the given system prompt and
        user prompt.

        Args:
            system_prompt (str): The system prompt to guide the model's
                                 behavior.
            prompt (str): The user prompt to generate a response for.

        Returns:
            str: The output generated by the model after processing the input.
        """
        output = ""
        for event in replicate.stream(
            self.model_path,
            input={
                "prompt": prompt,
                "temperature": self.temperature,
                "system_prompt": system_prompt,
                "length_penalty": 1,
                "max_new_tokens": 2500,
                "stop_sequences": "<|end_of_text|>,<|eot_id|>",
                "prompt_template": (
                    "<|begin_of_text|><|start_header_id|>system"
                    "<|end_header_id|>\n\n{system_prompt}"
                    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n"
                    "{prompt}<|eot_id|><|start_header_id|>assistant"
                    "<|end_header_id|>\n\n"
                ),
                "presence_penalty": 0,
            },
        ):
            output += str(event)
        return output


# Wrapper class to make OpenAI class function correctly
class CustomOpenAI(OpenAI):
    """
    A custom class extending the OpenAI class to modify or add functionality.

    Inherits from the OpenAI class and is used for preparing chats with tools.
    """

    def _prepare_chat_with_tools(self):
        pass


# GPT-4o model
gpt4 = CustomOpenAI(temperature=0.1, model="gpt-4o")

# Llama 3 8b for initial compleition
llama3_8b_interface = Model(
    model_path="meta/meta-llama-3-8b-instruct",
    temperature=0.1,
    additional_args={"max_new_tokens": 500},
    system_prompt=interface_form_completion_prompt,
)

# Llama 3 8d model for feedback
llama3_8b_feedback = FeedbackModel(
    model_path="meta/meta-llama-3-8b-instruct",
    temperature=0.1,
)

# Mixtral 7b
llama3_8b = Model(
    model_path="meta/meta-llama-3-8b-instruct",
    temperature=0.1,
    additional_args={"max_new_tokens": 2500},
    system_prompt=form_completion_prompt,
)

# Mixtral 7b
mixtral_7b = Model(
    model_path="mistralai/mixtral-8x7b-instruct-v0.1",
    temperature=0.1,
    additional_args={"max_new_tokens": 2500},
    system_prompt=form_completion_prompt,
)
